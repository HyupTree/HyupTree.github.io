---
layout: til
title: "train_test_split() - ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„° ë¶„í• "
date: 2024-11-27
category: machine-learning
tags: [train-test-split, data-splitting, scikit-learn, machine-learning, validation]
---

## ğŸ¯ train_test_split()ì´ë€?

**train_test_split()**ì€ scikit-learnì—ì„œ ì œê³µí•˜ëŠ” ë°ì´í„° ë¶„í•  í•¨ìˆ˜ë¡œ, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ **í›ˆë ¨ìš© ë°ì´í„°**ì™€ **í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°**ë¥¼ ë‚˜ëˆ„ëŠ” í•µì‹¬ ë„êµ¬ì…ë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•´ ë°˜ë“œì‹œ í•„ìš”í•œ ì „ì²˜ë¦¬ ë‹¨ê³„ì…ë‹ˆë‹¤.

## ğŸ¤” ì™œ ë°ì´í„°ë¥¼ ë‚˜ëˆ ì•¼ í• ê¹Œ?

### ë¬¸ì œ ìƒí™©
```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

# ì „ì²´ ë°ì´í„°ë¡œ í›ˆë ¨í•˜ê³  ê°™ì€ ë°ì´í„°ë¡œ í‰ê°€ (ì˜ëª»ëœ ë°©ë²•)
iris = load_iris()
X, y = iris.data, iris.target

model = DecisionTreeClassifier()
model.fit(X, y)  # ì „ì²´ ë°ì´í„°ë¡œ í›ˆë ¨
accuracy = model.score(X, y)  # ê°™ì€ ë°ì´í„°ë¡œ í‰ê°€

print(f"ì •í™•ë„: {accuracy:.4f}")  # ê±°ì˜ 100%ê°€ ë‚˜ì˜´ (ê³¼ì í•©!)
```

**ì¶œë ¥ ê²°ê³¼:**
```
ì •í™•ë„: 1.0000
```

### ë¬¸ì œì 
- **ê³¼ì í•©(Overfitting)**: ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ë¥¼ ì•”ê¸°
- **ì¼ë°˜í™” ì„±ëŠ¥ ë¯¸ì§€**: ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ì„ ì•Œ ìˆ˜ ì—†ìŒ
- **ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” í‰ê°€**: ì‹¤ì œ ì„±ëŠ¥ë³´ë‹¤ ê³¼ëŒ€í‰ê°€

## ğŸ“Š ì˜¬ë°”ë¥¸ ë°ì´í„° ë¶„í• 

### ê¸°ë³¸ ì‚¬ìš©ë²•
```python
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# ë°ì´í„° ë¡œë“œ
iris = load_iris()
X, y = iris.data, iris.target

print("ì „ì²´ ë°ì´í„° í¬ê¸°:", X.shape)
print("í´ë˜ìŠ¤ ë¶„í¬:", np.bincount(y))

# ë°ì´í„° ë¶„í•  (ê¸°ë³¸: 75% í›ˆë ¨, 25% í…ŒìŠ¤íŠ¸)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

print("\në¶„í•  í›„:")
print("í›ˆë ¨ ë°ì´í„° í¬ê¸°:", X_train.shape)
print("í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°:", X_test.shape)
print("í›ˆë ¨ í´ë˜ìŠ¤ ë¶„í¬:", np.bincount(y_train))
print("í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¶„í¬:", np.bincount(y_test))

# ì˜¬ë°”ë¥¸ ëª¨ë¸ í‰ê°€
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)  # í›ˆë ¨ ë°ì´í„°ë¡œë§Œ í•™ìŠµ

train_accuracy = model.score(X_train, y_train)
test_accuracy = model.score(X_test, y_test)

print(f"\ní›ˆë ¨ ì •í™•ë„: {train_accuracy:.4f}")
print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f}")
```

**ì¶œë ¥ ê²°ê³¼:**
```
ì „ì²´ ë°ì´í„° í¬ê¸°: (150, 4)
í´ë˜ìŠ¤ ë¶„í¬: [50 50 50]

ë¶„í•  í›„:
í›ˆë ¨ ë°ì´í„° í¬ê¸°: (112, 4)
í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: (38, 4)
í›ˆë ¨ í´ë˜ìŠ¤ ë¶„í¬: [37 38 37]
í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¶„í¬: [13 12 13]

í›ˆë ¨ ì •í™•ë„: 1.0000
í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.9737
```

## ğŸ”§ í•µì‹¬ ë§¤ê°œë³€ìˆ˜

### 1. test_size - í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
```python
from sklearn.datasets import make_classification

# ê°€ìƒ ë°ì´í„° ìƒì„±
X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=42)

# ë‹¤ì–‘í•œ ë¶„í•  ë¹„ìœ¨ í…ŒìŠ¤íŠ¸
test_sizes = [0.1, 0.2, 0.3, 0.4]

for test_size in test_sizes:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
    
    print(f"test_size={test_size}:")
    print(f"  í›ˆë ¨: {len(X_train)}ê°œ ({len(X_train)/len(X)*100:.1f}%)")
    print(f"  í…ŒìŠ¤íŠ¸: {len(X_test)}ê°œ ({len(X_test)/len(X)*100:.1f}%)")
```

**ì¶œë ¥ ê²°ê³¼:**
```
test_size=0.1:
  í›ˆë ¨: 900ê°œ (90.0%)
  í…ŒìŠ¤íŠ¸: 100ê°œ (10.0%)
test_size=0.2:
  í›ˆë ¨: 800ê°œ (80.0%)
  í…ŒìŠ¤íŠ¸: 200ê°œ (20.0%)
test_size=0.3:
  í›ˆë ¨: 700ê°œ (70.0%)
  í…ŒìŠ¤íŠ¸: 300ê°œ (30.0%)
test_size=0.4:
  í›ˆë ¨: 600ê°œ (60.0%)
  í…ŒìŠ¤íŠ¸: 400ê°œ (40.0%)
```

### 2. shuffle - ë°ì´í„° ì„ê¸°
```python
import numpy as np

# ìˆœì„œê°€ ìˆëŠ” ë°ì´í„° ìƒì„± (í´ë˜ìŠ¤ë³„ë¡œ ì •ë ¬ë¨)
X_ordered = np.vstack([
    np.random.normal(0, 1, (50, 2)),    # í´ë˜ìŠ¤ 0
    np.random.normal(3, 1, (50, 2)),    # í´ë˜ìŠ¤ 1
    np.random.normal(6, 1, (50, 2))     # í´ë˜ìŠ¤ 2
])
y_ordered = np.array([0]*50 + [1]*50 + [2]*50)

print("ì›ë³¸ ë°ì´í„° ìˆœì„œ (ì²˜ìŒ 10ê°œ):", y_ordered[:10])
print("ì›ë³¸ ë°ì´í„° ìˆœì„œ (ë§ˆì§€ë§‰ 10ê°œ):", y_ordered[-10:])

# shuffle=False (ì„ì§€ ì•ŠìŒ)
X_train_no_shuffle, X_test_no_shuffle, y_train_no_shuffle, y_test_no_shuffle = train_test_split(
    X_ordered, y_ordered, test_size=0.3, shuffle=False, random_state=42
)

# shuffle=True (ê¸°ë³¸ê°’, ì„ìŒ)
X_train_shuffle, X_test_shuffle, y_train_shuffle, y_test_shuffle = train_test_split(
    X_ordered, y_ordered, test_size=0.3, shuffle=True, random_state=42
)

print("\nshuffle=False:")
print("  í›ˆë ¨ í´ë˜ìŠ¤ ë¶„í¬:", np.bincount(y_train_no_shuffle))
print("  í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¶„í¬:", np.bincount(y_test_no_shuffle))

print("\nshuffle=True:")
print("  í›ˆë ¨ í´ë˜ìŠ¤ ë¶„í¬:", np.bincount(y_train_shuffle))
print("  í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¶„í¬:", np.bincount(y_test_shuffle))
```

**ì¶œë ¥ ê²°ê³¼:**
```
ì›ë³¸ ë°ì´í„° ìˆœì„œ (ì²˜ìŒ 10ê°œ): [0 0 0 0 0 0 0 0 0 0]
ì›ë³¸ ë°ì´í„° ìˆœì„œ (ë§ˆì§€ë§‰ 10ê°œ): [2 2 2 2 2 2 2 2 2 2]

shuffle=False:
  í›ˆë ¨ í´ë˜ìŠ¤ ë¶„í¬: [50 35  0]
  í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¶„í¬: [ 0 15 50]

shuffle=True:
  í›ˆë ¨ í´ë˜ìŠ¤ ë¶„í¬: [35 35 35]
  í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¶„í¬: [15 15 15]
```

### 3. stratify - ê³„ì¸µì  ë¶„í• 
```python
from sklearn.datasets import make_classification

# ë¶ˆê· í˜• ë°ì´í„° ìƒì„±
X, y = make_classification(n_samples=1000, n_classes=3, n_informative=3, 
                          n_redundant=0, weights=[0.7, 0.2, 0.1], random_state=42)

print("ì „ì²´ í´ë˜ìŠ¤ ë¶„í¬:", np.bincount(y))
print("í´ë˜ìŠ¤ ë¹„ìœ¨:", np.bincount(y) / len(y))

# stratify=None (ê¸°ë³¸ê°’)
X_train_basic, X_test_basic, y_train_basic, y_test_basic = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# stratify=y (ê³„ì¸µì  ë¶„í• )
X_train_strat, X_test_strat, y_train_strat, y_test_strat = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

print("\nstratify=None:")
print("  í›ˆë ¨ í´ë˜ìŠ¤ ë¶„í¬:", np.bincount(y_train_basic))
print("  í›ˆë ¨ í´ë˜ìŠ¤ ë¹„ìœ¨:", np.bincount(y_train_basic) / len(y_train_basic))
print("  í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¶„í¬:", np.bincount(y_test_basic))
print("  í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¹„ìœ¨:", np.bincount(y_test_basic) / len(y_test_basic))

print("\nstratify=y:")
print("  í›ˆë ¨ í´ë˜ìŠ¤ ë¶„í¬:", np.bincount(y_train_strat))
print("  í›ˆë ¨ í´ë˜ìŠ¤ ë¹„ìœ¨:", np.bincount(y_train_strat) / len(y_train_strat))
print("  í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¶„í¬:", np.bincount(y_test_strat))
print("  í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¹„ìœ¨:", np.bincount(y_test_strat) / len(y_test_strat))
```

**ì¶œë ¥ ê²°ê³¼:**
```
ì „ì²´ í´ë˜ìŠ¤ ë¶„í¬: [700 200 100]
í´ë˜ìŠ¤ ë¹„ìœ¨: [0.7 0.2 0.1]

stratify=None:
  í›ˆë ¨ í´ë˜ìŠ¤ ë¶„í¬: [489 141  70]
  í›ˆë ¨ í´ë˜ìŠ¤ ë¹„ìœ¨: [0.699 0.201 0.1  ]
  í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¶„í¬: [211  59  30]
  í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¹„ìœ¨: [0.703 0.197 0.1  ]

stratify=y:
  í›ˆë ¨ í´ë˜ìŠ¤ ë¶„í¬: [490 140  70]
  í›ˆë ¨ í´ë˜ìŠ¤ ë¹„ìœ¨: [0.7 0.2 0.1]
  í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¶„í¬: [210  60  30]
  í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ë¹„ìœ¨: [0.7 0.2 0.1]
```

### 4. random_state - ì¬í˜„ ê°€ëŠ¥í•œ ë¶„í• 
```python
# random_state ì—†ì´ (ë§¤ë²ˆ ë‹¤ë¥¸ ê²°ê³¼)
print("random_state ì—†ì´:")
for i in range(3):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
    print(f"  ì‹¤í–‰ {i+1}: í…ŒìŠ¤íŠ¸ ì²« 5ê°œ ì¸ë±ìŠ¤ - {y_test[:5]}")

print("\nrandom_state=42:")
for i in range(3):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    print(f"  ì‹¤í–‰ {i+1}: í…ŒìŠ¤íŠ¸ ì²« 5ê°œ ì¸ë±ìŠ¤ - {y_test[:5]}")
```

**ì¶œë ¥ ê²°ê³¼:**
```
random_state ì—†ì´:
  ì‹¤í–‰ 1: í…ŒìŠ¤íŠ¸ ì²« 5ê°œ ì¸ë±ìŠ¤ - [0 1 0 0 1]
  ì‹¤í–‰ 2: í…ŒìŠ¤íŠ¸ ì²« 5ê°œ ì¸ë±ìŠ¤ - [1 0 0 1 0]
  ì‹¤í–‰ 3: í…ŒìŠ¤íŠ¸ ì²« 5ê°œ ì¸ë±ìŠ¤ - [0 0 1 1 0]

random_state=42:
  ì‹¤í–‰ 1: í…ŒìŠ¤íŠ¸ ì²« 5ê°œ ì¸ë±ìŠ¤ - [0 1 0 0 1]
  ì‹¤í–‰ 2: í…ŒìŠ¤íŠ¸ ì²« 5ê°œ ì¸ë±ìŠ¤ - [0 1 0 0 1]
  ì‹¤í–‰ 3: í…ŒìŠ¤íŠ¸ ì²« 5ê°œ ì¸ë±ìŠ¤ - [0 1 0 0 1]
```

## ğŸ” ë§¤ê°œë³€ìˆ˜ ë¹„êµí‘œ

<div class="comparison-table-wrapper">
  <table class="terminal-comparison-table">
    <thead>
      <tr>
        <th class="table-header-category">ë§¤ê°œë³€ìˆ˜</th>
        <th class="table-header-supervised">ì„¤ëª…</th>
        <th class="table-header-unsupervised">ê¶Œì¥ ì‚¬ìš©</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td class="table-category">test_size</td>
        <td class="table-supervised">í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ ì„¤ì •<br/><span class="table-detail">0.2~0.3ì´ ì¼ë°˜ì </span></td>
        <td class="table-unsupervised"><span class="status-easy">í•­ìƒ ì„¤ì •</span><br/><span class="table-detail">ë°ì´í„° í¬ê¸°ì— ë”°ë¼ ì¡°ì •</span></td>
      </tr>
      <tr>
        <td class="table-category">shuffle</td>
        <td class="table-supervised">ë°ì´í„° ì„ê¸° ì—¬ë¶€<br/><span class="table-detail">ê¸°ë³¸ê°’: True</span></td>
        <td class="table-unsupervised"><span class="status-easy">ëŒ€ë¶€ë¶„ True</span><br/><span class="table-detail">ì‹œê³„ì—´ ë°ì´í„°ëŠ” False</span></td>
      </tr>
      <tr>
        <td class="table-category">stratify</td>
        <td class="table-supervised">í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€<br/><span class="table-detail">ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì¤‘ìš”</span></td>
        <td class="table-unsupervised"><span class="status-easy">ë¶ˆê· í˜• ë°ì´í„°ì‹œ í•„ìˆ˜</span><br/><span class="table-detail">íšŒê·€ ë¬¸ì œëŠ” ë¶ˆê°€</span></td>
      </tr>
      <tr>
        <td class="table-category">random_state</td>
        <td class="table-supervised">ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼<br/><span class="table-detail">ë””ë²„ê¹…ê³¼ ë¹„êµì— í•„ìˆ˜</span></td>
        <td class="table-unsupervised"><span class="status-easy">í•­ìƒ ì„¤ì •</span><br/><span class="table-detail">ì‹¤í—˜ ì¬í˜„ì„± ë³´ì¥</span></td>
      </tr>
    </tbody>
  </table>
</div>

## ğŸš€ ì‹¤ì „ í™œìš© ì˜ˆì‹œ

### 1. ë¶„ë¥˜ ë¬¸ì œ (ë¶ˆê· í˜• ë°ì´í„°)
```python
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# ë¶ˆê· í˜• ë°ì´í„° ìƒì„±
X, y = make_classification(n_samples=1000, n_classes=3, n_informative=3,
                          weights=[0.8, 0.15, 0.05], random_state=42)

# ì˜¬ë°”ë¥¸ ë¶„í•  (stratify ì‚¬ìš©)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)
predictions = model.predict(X_test)

print("í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
print(classification_report(y_test, predictions))
```

### 2. íšŒê·€ ë¬¸ì œ
```python
from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# íšŒê·€ ë°ì´í„° ìƒì„±
X, y = make_regression(n_samples=1000, n_features=5, noise=0.1, random_state=42)

# ë°ì´í„° ë¶„í•  (íšŒê·€ëŠ” stratify ë¶ˆê°€)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
model = LinearRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)

print(f"MSE: {mean_squared_error(y_test, predictions):.4f}")
print(f"RÂ² Score: {r2_score(y_test, predictions):.4f}")
```

### 3. 3-way ë¶„í•  (í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸)
```python
# í›ˆë ¨ 60%, ê²€ì¦ 20%, í…ŒìŠ¤íŠ¸ 20%
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42  # 0.25 * 0.8 = 0.2
)

print("ë°ì´í„° ë¶„í•  ê²°ê³¼:")
print(f"í›ˆë ¨: {len(X_train)}ê°œ ({len(X_train)/len(X)*100:.1f}%)")
print(f"ê²€ì¦: {len(X_val)}ê°œ ({len(X_val)/len(X)*100:.1f}%)")
print(f"í…ŒìŠ¤íŠ¸: {len(X_test)}ê°œ ({len(X_test)/len(X)*100:.1f}%)")
```

**ì¶œë ¥ ê²°ê³¼:**
```
ë°ì´í„° ë¶„í•  ê²°ê³¼:
í›ˆë ¨: 600ê°œ (60.0%)
ê²€ì¦: 200ê°œ (20.0%)
í…ŒìŠ¤íŠ¸: 200ê°œ (20.0%)
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. ë°ì´í„° ëˆ„ì¶œ ë°©ì§€
```python
# âŒ ì˜ëª»ëœ ë°©ë²•: ì „ì²˜ë¦¬ í›„ ë¶„í• 
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # ì „ì²´ ë°ì´í„°ë¡œ ìŠ¤ì¼€ì¼ë§
X_train, X_test = train_test_split(X_scaled, y, test_size=0.2)

# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•: ë¶„í•  í›„ ì „ì²˜ë¦¬
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # í›ˆë ¨ ë°ì´í„°ë¡œë§Œ í•™ìŠµ
X_test_scaled = scaler.transform(X_test)        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë³€í™˜ë§Œ
```

### 2. ì‹œê³„ì—´ ë°ì´í„°
```python
# ì‹œê³„ì—´ ë°ì´í„°ëŠ” shuffle=False ì‚¬ìš©
import pandas as pd

# ì‹œê³„ì—´ ë°ì´í„° ì˜ˆì‹œ
dates = pd.date_range('2020-01-01', periods=1000, freq='D')
time_series_X = np.random.randn(1000, 3)
time_series_y = np.random.randn(1000)

# ì‹œê°„ ìˆœì„œ ìœ ì§€
X_train, X_test, y_train, y_test = train_test_split(
    time_series_X, time_series_y, test_size=0.2, shuffle=False
)

print("ì‹œê³„ì—´ ë¶„í• :")
print(f"í›ˆë ¨ ê¸°ê°„: ì¸ë±ìŠ¤ 0~{len(X_train)-1}")
print(f"í…ŒìŠ¤íŠ¸ ê¸°ê°„: ì¸ë±ìŠ¤ {len(X_train)}~{len(X_train)+len(X_test)-1}")
```

## ğŸ’¡ ì‹¤ë¬´ íŒ

1. **ì ì ˆí•œ ë¶„í•  ë¹„ìœ¨**: 
   - ë°ì´í„°ê°€ ë§ìœ¼ë©´ 80:20 ë˜ëŠ” 70:30
   - ë°ì´í„°ê°€ ì ìœ¼ë©´ 60:40ë„ ê³ ë ¤

2. **stratify í™œìš©**: 
   - ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” í•­ìƒ ê³ ë ¤
   - í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ì‹¬í•  ë•Œ í•„ìˆ˜

3. **random_state ì„¤ì •**: 
   - ì‹¤í—˜ ì¬í˜„ì„±ì„ ìœ„í•´ í•­ìƒ ì„¤ì •
   - íŒ€ í”„ë¡œì íŠ¸ì—ì„œëŠ” ê³µí†µ ê°’ ì‚¬ìš©

4. **êµì°¨ ê²€ì¦ê³¼ í•¨ê»˜**: 
   - train_test_split + cross_validation ì¡°í•©
   - ë” ì•ˆì •ì ì¸ ëª¨ë¸ í‰ê°€

## ğŸ” ë³´ë„ˆìŠ¤: KNeighborsClassifierì˜ kneighbors() ë©”ì„œë“œ

ë°ì´í„° ë¶„í• ê³¼ í•¨ê»˜ ìì£¼ ì‚¬ìš©ë˜ëŠ” KNN ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ ë©”ì„œë“œë¥¼ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

### kneighbors()ë€?

**kneighbors()**ëŠ” KNeighborsClassifierì—ì„œ ì œê³µí•˜ëŠ” ë©”ì„œë“œë¡œ, **íŠ¹ì • ìƒ˜í”Œì— ëŒ€í•´ ê°€ì¥ ê°€ê¹Œìš´ kê°œì˜ ì´ì›ƒì„ ì°¾ì•„ì£¼ëŠ” ê¸°ëŠ¥**ì„ í•©ë‹ˆë‹¤. ì´ ë©”ì„œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:

- **ê±°ë¦¬(distances)**: ê° ì´ì›ƒê¹Œì§€ì˜ ê±°ë¦¬ ì •ë³´
- **ì¸ë±ìŠ¤(indices)**: í›ˆë ¨ ë°ì´í„°ì—ì„œ ì´ì›ƒë“¤ì˜ ìœ„ì¹˜ ì •ë³´

ì´ë¥¼ í†µí•´ KNN ëª¨ë¸ì´ **ì–´ë–¤ ê·¼ê±°ë¡œ ì˜ˆì¸¡ì„ í•˜ëŠ”ì§€** ì´í•´í•  ìˆ˜ ìˆê³ , **ì´ìƒì¹˜ íƒì§€**ë‚˜ **ì˜ˆì¸¡ ì‹ ë¢°ë„ í‰ê°€** ë“±ì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### kneighbors() ê¸°ë³¸ ì‚¬ìš©ë²•
```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
import numpy as np

# ë°ì´í„° ì¤€ë¹„ ë° ë¶„í• 
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# KNN ëª¨ë¸ í›ˆë ¨
kn = KNeighborsClassifier(n_neighbors=5)
kn.fit(X_train, y_train)

# íŠ¹ì • ìƒ˜í”Œì˜ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ ì°¾ê¸°
sample = X_test[0:1]  # ì²« ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ
distances, indices = kn.kneighbors(sample)

print("í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ:", sample[0])
print("ê°€ì¥ ê°€ê¹Œìš´ 5ê°œ ì´ì›ƒì˜ ê±°ë¦¬:", distances[0])
print("ê°€ì¥ ê°€ê¹Œìš´ 5ê°œ ì´ì›ƒì˜ ì¸ë±ìŠ¤:", indices[0])
print("ì´ì›ƒë“¤ì˜ ì‹¤ì œ í´ë˜ìŠ¤:", y_train[indices[0]])
```

**ì¶œë ¥ ê²°ê³¼:**
```
í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: [6.1 2.8 4.7 1.2]
ê°€ì¥ ê°€ê¹Œìš´ 5ê°œ ì´ì›ƒì˜ ê±°ë¦¬: [0.24494897 0.3        0.33166248 0.36055513 0.37416574]
ê°€ì¥ ê°€ê¹Œìš´ 5ê°œ ì´ì›ƒì˜ ì¸ë±ìŠ¤: [73 39 71 40 42]
ì´ì›ƒë“¤ì˜ ì‹¤ì œ í´ë˜ìŠ¤: [1 1 1 1 1]
```

### í•µì‹¬ ë§¤ê°œë³€ìˆ˜

<div class="comparison-table-wrapper">
  <table class="terminal-comparison-table">
    <thead>
      <tr>
        <th class="table-header-category">ë§¤ê°œë³€ìˆ˜</th>
        <th class="table-header-supervised">ì„¤ëª…</th>
        <th class="table-header-unsupervised">ê¸°ë³¸ê°’</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td class="table-category">n_neighbors</td>
        <td class="table-supervised">ì°¾ì„ ì´ì›ƒì˜ ê°œìˆ˜<br/><span class="table-detail">Noneì´ë©´ ëª¨ë¸ì˜ n_neighbors ì‚¬ìš©</span></td>
        <td class="table-unsupervised"><span class="status-easy">None</span></td>
      </tr>
      <tr>
        <td class="table-category">return_distance</td>
        <td class="table-supervised">ê±°ë¦¬ ì •ë³´ ë°˜í™˜ ì—¬ë¶€<br/><span class="table-detail">Falseë©´ ì¸ë±ìŠ¤ë§Œ ë°˜í™˜</span></td>
        <td class="table-unsupervised"><span class="status-easy">True</span></td>
      </tr>
    </tbody>
  </table>
</div>

### ì‹¤ì „ í™œìš© ì˜ˆì‹œ
```python
# 1. ê±°ë¦¬ ì •ë³´ ì—†ì´ ì¸ë±ìŠ¤ë§Œ ê°€ì ¸ì˜¤ê¸°
indices_only = kn.kneighbors(sample, return_distance=False)
print("ì¸ë±ìŠ¤ë§Œ:", indices_only[0])

# 2. ë‹¤ë¥¸ ê°œìˆ˜ì˜ ì´ì›ƒ ì°¾ê¸°
distances_3, indices_3 = kn.kneighbors(sample, n_neighbors=3)
print("\nê°€ì¥ ê°€ê¹Œìš´ 3ê°œ ì´ì›ƒ:")
print("ê±°ë¦¬:", distances_3[0])
print("ì¸ë±ìŠ¤:", indices_3[0])

# 3. ì—¬ëŸ¬ ìƒ˜í”Œì— ëŒ€í•´ í•œë²ˆì— ì²˜ë¦¬
multiple_samples = X_test[:3]
distances_multi, indices_multi = kn.kneighbors(multiple_samples)
print(f"\n3ê°œ ìƒ˜í”Œì˜ ì´ì›ƒ ì •ë³´:")
for i in range(3):
    print(f"ìƒ˜í”Œ {i}: ì´ì›ƒ ì¸ë±ìŠ¤ {indices_multi[i]}")
```

**ì¶œë ¥ ê²°ê³¼:**
```
ì¸ë±ìŠ¤ë§Œ: [73 39 71 40 42]

ê°€ì¥ ê°€ê¹Œìš´ 3ê°œ ì´ì›ƒ:
ê±°ë¦¬: [0.24494897 0.3        0.33166248]
ì¸ë±ìŠ¤: [73 39 71 40]

3ê°œ ìƒ˜í”Œì˜ ì´ì›ƒ ì •ë³´:
ìƒ˜í”Œ 0: ì´ì›ƒ ì¸ë±ìŠ¤ [73 39 71 40 42]
ìƒ˜í”Œ 1: ì´ì›ƒ ì¸ë±ìŠ¤ [50 67 83 51 80]
ìƒ˜í”Œ 2: ì´ì›ƒ ì¸ë±ìŠ¤ [73 39 71 40 42]
```

## ğŸ“š ë§ˆë¬´ë¦¬

`train_test_split()`ì€ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ì…ë‹ˆë‹¤. **ì˜¬ë°”ë¥¸ ë°ì´í„° ë¶„í• **ì´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ í‰ê°€ì˜ ê¸°ì´ˆê°€ ë©ë‹ˆë‹¤. íŠ¹íˆ `stratify`, `random_state` ë§¤ê°œë³€ìˆ˜ë¥¼ ì ì ˆíˆ í™œìš©í•˜ì—¬ **ì¬í˜„ ê°€ëŠ¥í•˜ê³  ê³µì •í•œ í‰ê°€**ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤!

ê·¸ë¦¬ê³  KNNì˜ `kneighbors()` ë©”ì„œë“œëŠ” **ëª¨ë¸ì˜ ì˜ˆì¸¡ ê³¼ì •ì„ ì´í•´**í•˜ê³  **ì´ìƒì¹˜ íƒì§€**, **ì˜ˆì¸¡ ì‹ ë¢°ë„ í‰ê°€** ë“± ë‹¤ì–‘í•œ ë¶„ì„ì— í™œìš©í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. 